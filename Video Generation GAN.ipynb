{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th') # ensure our dimension notation matches\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Reshape\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D , Convolution2D, AveragePooling2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.datasets import mnist\n",
    "from PIL import Image,ImageOps\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "import imageio #to generate gif or video\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Container.summary of <keras.models.Sequential object at 0x7f4a3cdc8eb8>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), input_shape=(1, 128, 1..., padding=\"same\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (5, 5))`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def get_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(64,5,5,border_mode = \"same\",input_shape =(1,128,128)))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(AveragePooling2D(pool_size=(4,4)))\n",
    "    model.add(Convolution2D(128,5,5))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(AveragePooling2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model\n",
    "print(get_discriminator().summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Container.summary of <keras.models.Sequential object at 0x7f4a890390f0>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), input_shape=(1, 128, 1..., padding=\"same\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (5, 5))`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(get_discriminator().summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=100,output_dim = 1024))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(128*8*8))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Reshape((128,8,8),input_shape = (128*8*8,)))\n",
    "    model.add(UpSampling2D(size=(4,4)))\n",
    "    model.add(Convolution2D(64,5,5,border_mode=\"same\"))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(UpSampling2D(size=(4,4)))\n",
    "    model.add(Convolution2D(1,5,5,border_mode=\"same\"))\n",
    "    model.add(Activation('tanh'))\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GAN(generator,discriminator):\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    discriminator.trainable = False\n",
    "    model.add(discriminator)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_data(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[2:]\n",
    "    image = np.zeros((height*shape[0],width*shape[1]),dtype=generated_images.dtype)\n",
    "    \n",
    "    for index,img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index %width\n",
    "        image[i*shape[0]:(i+1)*shape[0],j*shape[1]:(j+1)*shape[1]] = img[0,:,:]\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(pixels = 128):\n",
    "    print('Loading data')\n",
    "    X_train = []\n",
    "    paths = glob.glob(os.path.normpath(os.getcwd()+\"/logos/*.jpg\"))\n",
    "    loaded_count = 0\n",
    "    \n",
    "    for path in paths:\n",
    "        image = Image.open(path)\n",
    "        image = ImageOps.fit(image,(pixels,pixels),Image.ANTIALIAS)\n",
    "        image = ImageOps.grayscale(image)\n",
    "        image = np.asarray(image)\n",
    "        X_train.append(image)\n",
    "        \n",
    "        loaded_count += 1\n",
    "        \n",
    "        if loaded_count%10 ==0:\n",
    "            print('Loaded '+str(loaded_count)+ \" images \")\n",
    "        \n",
    "    print(\"Finished loading data\")\n",
    "        \n",
    "    return np.array(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Loaded 10 images \n",
      "Loaded 20 images \n",
      "Loaded 30 images \n",
      "Finished loading data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(38, 128, 128)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(load_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_GAN(epochs,batch_size,load_weights= False,learning_rate = .0005):\n",
    "    \"\"\"\n",
    "    :param epochs: Number of training epochs\n",
    "    :param batch_size: size of minibatch\n",
    "    :param load_weights: If true, load trained weights from file, else train from scratch\n",
    "    \"\"\"\n",
    "    X_train = load_data()\n",
    "    X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1) + X_train.shape[1:])\n",
    "    \n",
    "    discriminator = get_discriminator()\n",
    "    generator = get_generator()\n",
    "    \n",
    "    if load_weights:\n",
    "        generator.load_weights('goodgenerator.h5')\n",
    "        discriminator.load_weights('gooddiscriminator.h5')\n",
    "        \n",
    "    GAN_model = GAN(generator,discriminator)\n",
    "    \n",
    "    d_optimizer = SGD(lr=learning_rate,momentum=0.9,nesterov=True)\n",
    "    g_optimizer = SGD(lr=learning_rate,momentum=0.9,nesterov=True)\n",
    "    \n",
    "    generator.compile(loss =\"binary_crossentropy\",optimizer=\"SGD\")\n",
    "    GAN_model.compile(loss=\"binary_crossentropy\",optimizer=g_optimizer)\n",
    "    discriminator.trainable = True\n",
    "    discriminator.compile(loss=\"binary_crossentropy\",optimizer=d_optimizer)\n",
    "    \n",
    "    noise = np.zeros((batch_size,100))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for index in range(int(X_train.shape[0]/batch_size)):\n",
    "            \n",
    "            #generate a batch of random noise to generator input\n",
    "            for i in range(batch_size):\n",
    "                noise[i,:] = np.random.uniform(-1,1,100)\n",
    "                \n",
    "            image_batch = X_train[index*batch_size:(index+1)*batch_size]\n",
    "            generated_images = generator.predict(noise,verbose = 0)\n",
    "            \n",
    "            \n",
    "            # create a train batch containing generated and real images together\n",
    "            X = np.concatenate((image_batch,generated_images))\n",
    "            y = [0.9] * batch_size + [0] * batch_size #use 0.9 instead of 1 for label smoothing\n",
    "            \n",
    "            #train discriminator\n",
    "            d_loss = discriminator.train_on_batch(X,y)\n",
    "            \n",
    "            # once in a while write generated images and write stats\n",
    "            if index % 20 == 0 and epoch % 10 == 0:\n",
    "                image = format_data(generated_images)\n",
    "                image = image*127.5+127.5\n",
    "                destpath = os.path.normpath(os.getcwd()+ \"/logo-generated-images/\"+str(epoch)+\"_\"+str(index)+\".png\")\n",
    "                Image.fromarray(image.astype(np.uint8)).save(destpath)\n",
    "                print(\"batch %d d_loss : %f\" % (epoch, d_loss))\n",
    "            \n",
    "            # Generate generator training set\n",
    "            ### generate images from random noise\n",
    "            for i in range(batch_size):\n",
    "                noise[i,:] = np.random.uniform(-1,1,100)\n",
    "                \n",
    "           ### generate labels\n",
    "            g_y = [1] * batch_size\n",
    "            \n",
    "            discriminator.trainable = False\n",
    "            g_loss = GAN_model.train_on_batch(noise,g_y)\n",
    "            discriminator.trainable = True\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                generator.save_weights('goodgenerator.h5', True)\n",
    "                discriminator.save_weights('gooddiscriminator.h5', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_image(image):\n",
    "    for i in range(1,image.shape[0] - 1):\n",
    "        for j in range(1, image.shape[1] - 1):\n",
    "            if image[i][j] + image[i+1][j] + image[i][j+1] + image[i-1][j] + image[i][j-1] > 127 * 5:\n",
    "                image[i][j] = 255\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_image_batch(batch_size):\n",
    "    generator  = get_generator()\n",
    "    generator.compile(loss=\"binary_crossentropy\",optimizer=\"SGD\")\n",
    "    generator.load_weights('goodgenerator.h5')\n",
    "    \n",
    "    noise = np.zeros((batch_size,100))\n",
    "    a = np.random.uniform(-1, 1, 100)\n",
    "    b = np.random.uniform(-1, 1, 100)\n",
    "    grad = (b - a) / batch_size\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        noise[i, :] = np.random.uniform(-1, 1, 100)\n",
    "    generated_images = generator.predict(noise, verbose=1)\n",
    "    \n",
    "    images  = []\n",
    "    print(generated_images.shape)\n",
    "    for image in generated_images:\n",
    "        image = image[0]\n",
    "        image = image*127.5+127.5\n",
    "        Image.fromarray(image.astype(np.uint8)).save(\"dirty.png\")\n",
    "        #Image.fromarray(image.astype(np.uint8)).show()\n",
    "        clean_image(image)\n",
    "        image = Image.fromarray(image.astype(np.uint8))\n",
    "        #image.show()        \n",
    "        image.save(\"clean.png\")\n",
    "        #np.reshape(list(images[0].getdata()),(128,128))\n",
    "        images.append(np.reshape(list(image.getdata()),(128,128)))\n",
    "        \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Loaded 10 images \n",
      "Loaded 20 images \n",
      "Loaded 30 images \n",
      "Finished loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), input_shape=(1, 128, 1..., padding=\"same\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (5, 5))`\n",
      "  \n",
      "/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=100, units=1024)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), padding=\"same\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (5, 5), padding=\"same\")`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 d_loss : 0.725251\n",
      "batch 10 d_loss : 0.555442\n",
      "batch 20 d_loss : 0.440450\n",
      "batch 30 d_loss : 0.317443\n",
      "batch 40 d_loss : 0.264587\n",
      "batch 50 d_loss : 0.311017\n",
      "batch 60 d_loss : 0.354417\n",
      "batch 70 d_loss : 0.477690\n",
      "batch 80 d_loss : 0.481839\n",
      "batch 90 d_loss : 0.370653\n",
      "batch 100 d_loss : 0.427865\n",
      "batch 110 d_loss : 0.448228\n",
      "batch 120 d_loss : 0.507761\n",
      "batch 130 d_loss : 0.522686\n",
      "batch 140 d_loss : 0.526671\n",
      "batch 150 d_loss : 0.494315\n",
      "batch 160 d_loss : 0.544047\n",
      "batch 170 d_loss : 0.431108\n",
      "batch 180 d_loss : 0.538872\n",
      "batch 190 d_loss : 0.576528\n",
      "batch 200 d_loss : 0.479926\n",
      "batch 210 d_loss : 0.476279\n",
      "batch 220 d_loss : 0.432606\n",
      "batch 230 d_loss : 0.451101\n",
      "batch 240 d_loss : 0.458272\n",
      "batch 250 d_loss : 0.428243\n",
      "batch 260 d_loss : 0.423964\n",
      "batch 270 d_loss : 0.491193\n",
      "batch 280 d_loss : 0.355426\n",
      "batch 290 d_loss : 0.404351\n",
      "batch 300 d_loss : 0.369495\n",
      "batch 310 d_loss : 0.381597\n",
      "batch 320 d_loss : 0.373415\n",
      "batch 330 d_loss : 0.315240\n",
      "batch 340 d_loss : 0.336963\n",
      "batch 350 d_loss : 0.380814\n",
      "batch 360 d_loss : 0.310400\n",
      "batch 370 d_loss : 0.356342\n",
      "batch 380 d_loss : 0.260606\n",
      "batch 390 d_loss : 0.319665\n",
      "batch 400 d_loss : 0.319282\n",
      "batch 410 d_loss : 0.290432\n",
      "batch 420 d_loss : 0.290869\n",
      "batch 430 d_loss : 0.257965\n",
      "batch 440 d_loss : 0.260715\n",
      "batch 450 d_loss : 0.259808\n",
      "batch 460 d_loss : 0.286882\n",
      "batch 470 d_loss : 0.209678\n",
      "batch 480 d_loss : 0.299906\n",
      "batch 490 d_loss : 0.244368\n",
      "batch 500 d_loss : 0.239494\n",
      "batch 510 d_loss : 0.247218\n",
      "batch 520 d_loss : 0.218178\n",
      "batch 530 d_loss : 0.280773\n",
      "batch 540 d_loss : 0.223105\n",
      "batch 550 d_loss : 0.220080\n",
      "batch 560 d_loss : 0.212550\n",
      "batch 570 d_loss : 0.228422\n",
      "batch 580 d_loss : 0.254383\n",
      "batch 590 d_loss : 0.232621\n",
      "batch 600 d_loss : 0.299666\n",
      "batch 610 d_loss : 0.269852\n",
      "batch 620 d_loss : 0.254087\n",
      "batch 630 d_loss : 0.231439\n",
      "batch 640 d_loss : 0.286676\n",
      "batch 650 d_loss : 0.209717\n",
      "batch 660 d_loss : 0.205998\n",
      "batch 670 d_loss : 0.239107\n",
      "batch 680 d_loss : 0.286627\n",
      "batch 690 d_loss : 0.200664\n",
      "batch 700 d_loss : 0.256513\n",
      "batch 710 d_loss : 0.251833\n",
      "batch 720 d_loss : 0.213714\n",
      "batch 730 d_loss : 0.266567\n",
      "batch 740 d_loss : 0.222637\n",
      "batch 750 d_loss : 0.221510\n",
      "batch 760 d_loss : 0.361114\n",
      "batch 770 d_loss : 0.237909\n",
      "batch 780 d_loss : 0.286617\n",
      "batch 790 d_loss : 0.239031\n",
      "batch 800 d_loss : 0.239244\n",
      "batch 810 d_loss : 0.236105\n",
      "batch 820 d_loss : 0.259537\n",
      "batch 830 d_loss : 0.231029\n",
      "batch 840 d_loss : 0.230613\n",
      "batch 850 d_loss : 0.326944\n",
      "batch 860 d_loss : 0.272000\n",
      "batch 870 d_loss : 0.260355\n",
      "batch 880 d_loss : 0.236685\n",
      "batch 890 d_loss : 0.260970\n",
      "batch 900 d_loss : 0.241995\n",
      "batch 910 d_loss : 0.236917\n",
      "batch 920 d_loss : 0.219973\n",
      "batch 930 d_loss : 0.219299\n",
      "batch 940 d_loss : 0.231071\n",
      "batch 950 d_loss : 0.272795\n",
      "batch 960 d_loss : 0.249062\n",
      "batch 970 d_loss : 0.239257\n",
      "batch 980 d_loss : 0.238880\n",
      "batch 990 d_loss : 0.270997\n",
      "batch 1000 d_loss : 0.290957\n",
      "batch 1010 d_loss : 0.272193\n",
      "batch 1020 d_loss : 0.257311\n",
      "batch 1030 d_loss : 0.270733\n",
      "batch 1040 d_loss : 0.286261\n",
      "batch 1050 d_loss : 0.314195\n",
      "batch 1060 d_loss : 0.253147\n",
      "batch 1070 d_loss : 0.257453\n",
      "batch 1080 d_loss : 0.240090\n",
      "batch 1090 d_loss : 0.246321\n",
      "batch 1100 d_loss : 0.222354\n",
      "batch 1110 d_loss : 0.266137\n",
      "batch 1120 d_loss : 0.244102\n",
      "batch 1130 d_loss : 0.249376\n",
      "batch 1140 d_loss : 0.268174\n",
      "batch 1150 d_loss : 0.230652\n",
      "batch 1160 d_loss : 0.255163\n",
      "batch 1170 d_loss : 0.262396\n",
      "batch 1180 d_loss : 0.270127\n",
      "batch 1190 d_loss : 0.259572\n",
      "batch 1200 d_loss : 0.263680\n",
      "batch 1210 d_loss : 0.319597\n",
      "batch 1220 d_loss : 0.264777\n",
      "batch 1230 d_loss : 0.298284\n",
      "batch 1240 d_loss : 0.251875\n",
      "batch 1250 d_loss : 0.264050\n",
      "batch 1260 d_loss : 0.267924\n",
      "batch 1270 d_loss : 0.241424\n",
      "batch 1280 d_loss : 0.268525\n",
      "batch 1290 d_loss : 0.266335\n",
      "batch 1300 d_loss : 0.280577\n",
      "batch 1310 d_loss : 0.263496\n",
      "batch 1320 d_loss : 0.229759\n",
      "batch 1330 d_loss : 0.302112\n",
      "batch 1340 d_loss : 0.227910\n",
      "batch 1350 d_loss : 0.269691\n",
      "batch 1360 d_loss : 0.251426\n",
      "batch 1370 d_loss : 0.251698\n",
      "batch 1380 d_loss : 0.283169\n",
      "batch 1390 d_loss : 0.289418\n",
      "batch 1400 d_loss : 0.263456\n",
      "batch 1410 d_loss : 0.239005\n",
      "batch 1420 d_loss : 0.319132\n",
      "batch 1430 d_loss : 0.274015\n",
      "batch 1440 d_loss : 0.266628\n",
      "batch 1450 d_loss : 0.251910\n",
      "batch 1460 d_loss : 0.300330\n",
      "batch 1470 d_loss : 0.314494\n",
      "batch 1480 d_loss : 0.231671\n",
      "batch 1490 d_loss : 0.276776\n",
      "batch 1500 d_loss : 0.277691\n",
      "batch 1510 d_loss : 0.301806\n",
      "batch 1520 d_loss : 0.276999\n",
      "batch 1530 d_loss : 0.278685\n",
      "batch 1540 d_loss : 0.287178\n",
      "batch 1550 d_loss : 0.290425\n",
      "batch 1560 d_loss : 0.261825\n",
      "batch 1570 d_loss : 0.277326\n",
      "batch 1580 d_loss : 0.309184\n",
      "batch 1590 d_loss : 0.247295\n",
      "batch 1600 d_loss : 0.275666\n",
      "batch 1610 d_loss : 0.269067\n",
      "batch 1620 d_loss : 0.307154\n",
      "batch 1630 d_loss : 0.296086\n",
      "batch 1640 d_loss : 0.303675\n",
      "batch 1650 d_loss : 0.280501\n",
      "batch 1660 d_loss : 0.322750\n",
      "batch 1670 d_loss : 0.244889\n",
      "batch 1680 d_loss : 0.295072\n",
      "batch 1690 d_loss : 0.280994\n",
      "batch 1700 d_loss : 0.295535\n",
      "batch 1710 d_loss : 0.300169\n",
      "batch 1720 d_loss : 0.292424\n",
      "batch 1730 d_loss : 0.319821\n",
      "batch 1740 d_loss : 0.340137\n",
      "batch 1750 d_loss : 0.248332\n",
      "batch 1760 d_loss : 0.318224\n",
      "batch 1770 d_loss : 0.297967\n",
      "batch 1780 d_loss : 0.282131\n",
      "batch 1790 d_loss : 0.282035\n",
      "batch 1800 d_loss : 0.282256\n",
      "batch 1810 d_loss : 0.315204\n",
      "batch 1820 d_loss : 0.264152\n",
      "batch 1830 d_loss : 0.297782\n",
      "batch 1840 d_loss : 0.271196\n",
      "batch 1850 d_loss : 0.312175\n",
      "batch 1860 d_loss : 0.300430\n",
      "batch 1870 d_loss : 0.300901\n",
      "batch 1880 d_loss : 0.291740\n",
      "batch 1890 d_loss : 0.320681\n",
      "batch 1900 d_loss : 0.323349\n",
      "batch 1910 d_loss : 0.305528\n",
      "batch 1920 d_loss : 0.302555\n",
      "batch 1930 d_loss : 0.344182\n",
      "batch 1940 d_loss : 0.352471\n",
      "batch 1950 d_loss : 0.276511\n",
      "batch 1960 d_loss : 0.281365\n",
      "batch 1970 d_loss : 0.340228\n",
      "batch 1980 d_loss : 0.344006\n",
      "batch 1990 d_loss : 0.277949\n",
      "batch 2000 d_loss : 0.349323\n",
      "batch 2010 d_loss : 0.350928\n",
      "batch 2020 d_loss : 0.348867\n",
      "batch 2030 d_loss : 0.344629\n",
      "batch 2040 d_loss : 0.276600\n",
      "batch 2050 d_loss : 0.325136\n",
      "batch 2060 d_loss : 0.317155\n",
      "batch 2070 d_loss : 0.291654\n",
      "batch 2080 d_loss : 0.357843\n",
      "batch 2090 d_loss : 0.326631\n",
      "batch 2100 d_loss : 0.287817\n",
      "batch 2110 d_loss : 0.345970\n",
      "batch 2120 d_loss : 0.312737\n",
      "batch 2130 d_loss : 0.344953\n",
      "batch 2140 d_loss : 0.335682\n",
      "batch 2150 d_loss : 0.341188\n",
      "batch 2160 d_loss : 0.316754\n",
      "batch 2170 d_loss : 0.332048\n",
      "batch 2180 d_loss : 0.278086\n",
      "batch 2190 d_loss : 0.357181\n",
      "batch 2200 d_loss : 0.312138\n",
      "batch 2210 d_loss : 0.327977\n",
      "batch 2220 d_loss : 0.339516\n",
      "batch 2230 d_loss : 0.321996\n",
      "batch 2240 d_loss : 0.323347\n",
      "batch 2250 d_loss : 0.309330\n",
      "batch 2260 d_loss : 0.341573\n",
      "batch 2270 d_loss : 0.340753\n",
      "batch 2280 d_loss : 0.311184\n",
      "batch 2290 d_loss : 0.346200\n",
      "batch 2300 d_loss : 0.343370\n",
      "batch 2310 d_loss : 0.337331\n",
      "batch 2320 d_loss : 0.329963\n",
      "batch 2330 d_loss : 0.360263\n",
      "batch 2340 d_loss : 0.337495\n",
      "batch 2350 d_loss : 0.329616\n",
      "batch 2360 d_loss : 0.375108\n",
      "batch 2370 d_loss : 0.336298\n",
      "batch 2380 d_loss : 0.339156\n",
      "batch 2390 d_loss : 0.345673\n",
      "batch 2400 d_loss : 0.360858\n",
      "batch 2410 d_loss : 0.351231\n",
      "batch 2420 d_loss : 0.345918\n",
      "batch 2430 d_loss : 0.348310\n",
      "batch 2440 d_loss : 0.305150\n",
      "batch 2450 d_loss : 0.307243\n",
      "batch 2460 d_loss : 0.343615\n",
      "batch 2470 d_loss : 0.318856\n",
      "batch 2480 d_loss : 0.318169\n",
      "batch 2490 d_loss : 0.394379\n",
      "batch 2500 d_loss : 0.326422\n",
      "batch 2510 d_loss : 0.333458\n",
      "batch 2520 d_loss : 0.392198\n",
      "batch 2530 d_loss : 0.321787\n",
      "batch 2540 d_loss : 0.430342\n",
      "batch 2550 d_loss : 0.353344\n",
      "batch 2560 d_loss : 0.384990\n",
      "batch 2570 d_loss : 0.329097\n",
      "batch 2580 d_loss : 0.292956\n",
      "batch 2590 d_loss : 0.372086\n",
      "batch 2600 d_loss : 0.329790\n",
      "batch 2610 d_loss : 0.324793\n",
      "batch 2620 d_loss : 0.365109\n",
      "batch 2630 d_loss : 0.298819\n",
      "batch 2640 d_loss : 0.330546\n",
      "batch 2650 d_loss : 0.383731\n",
      "batch 2660 d_loss : 0.334580\n",
      "batch 2670 d_loss : 0.318902\n",
      "batch 2680 d_loss : 0.349979\n",
      "batch 2690 d_loss : 0.354589\n",
      "batch 2700 d_loss : 0.308869\n",
      "batch 2710 d_loss : 0.287370\n",
      "batch 2720 d_loss : 0.331984\n",
      "batch 2730 d_loss : 0.353570\n",
      "batch 2740 d_loss : 0.340491\n",
      "batch 2750 d_loss : 0.347171\n",
      "batch 2760 d_loss : 0.301272\n",
      "batch 2770 d_loss : 0.318835\n",
      "batch 2780 d_loss : 0.291650\n",
      "batch 2790 d_loss : 0.326621\n",
      "batch 2800 d_loss : 0.369655\n",
      "batch 2810 d_loss : 0.359068\n",
      "batch 2820 d_loss : 0.306984\n",
      "batch 2830 d_loss : 0.303419\n",
      "batch 2840 d_loss : 0.297456\n",
      "batch 2850 d_loss : 0.309055\n",
      "batch 2860 d_loss : 0.322578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2870 d_loss : 0.307616\n",
      "batch 2880 d_loss : 0.308306\n",
      "batch 2890 d_loss : 0.349573\n",
      "batch 2900 d_loss : 0.339203\n",
      "batch 2910 d_loss : 0.344188\n",
      "batch 2920 d_loss : 0.342477\n",
      "batch 2930 d_loss : 0.323920\n",
      "batch 2940 d_loss : 0.381509\n",
      "batch 2950 d_loss : 0.315908\n",
      "batch 2960 d_loss : 0.363017\n",
      "batch 2970 d_loss : 0.353792\n",
      "batch 2980 d_loss : 0.360410\n",
      "batch 2990 d_loss : 0.364389\n",
      "batch 3000 d_loss : 0.371027\n",
      "batch 3010 d_loss : 0.310648\n",
      "batch 3020 d_loss : 0.369654\n",
      "batch 3030 d_loss : 0.368253\n",
      "batch 3040 d_loss : 0.356877\n",
      "batch 3050 d_loss : 0.319551\n",
      "batch 3060 d_loss : 0.370363\n",
      "batch 3070 d_loss : 0.359636\n",
      "batch 3080 d_loss : 0.326055\n",
      "batch 3090 d_loss : 0.349940\n",
      "batch 3100 d_loss : 0.338333\n",
      "batch 3110 d_loss : 0.353690\n",
      "batch 3120 d_loss : 0.354014\n",
      "batch 3130 d_loss : 0.366173\n",
      "batch 3140 d_loss : 0.348814\n",
      "batch 3150 d_loss : 0.366450\n",
      "batch 3160 d_loss : 0.305791\n",
      "batch 3170 d_loss : 0.340693\n",
      "batch 3180 d_loss : 0.325447\n",
      "batch 3190 d_loss : 0.371218\n",
      "batch 3200 d_loss : 0.345484\n",
      "batch 3210 d_loss : 0.327515\n",
      "batch 3220 d_loss : 0.350450\n",
      "batch 3230 d_loss : 0.339370\n",
      "batch 3240 d_loss : 0.331497\n",
      "batch 3250 d_loss : 0.352090\n",
      "batch 3260 d_loss : 0.332352\n",
      "batch 3270 d_loss : 0.348129\n",
      "batch 3280 d_loss : 0.344185\n",
      "batch 3290 d_loss : 0.335119\n",
      "batch 3300 d_loss : 0.341202\n",
      "batch 3310 d_loss : 0.346590\n",
      "batch 3320 d_loss : 0.387823\n",
      "batch 3330 d_loss : 0.340322\n",
      "batch 3340 d_loss : 0.295875\n",
      "batch 3350 d_loss : 0.355205\n",
      "batch 3360 d_loss : 0.327017\n",
      "batch 3370 d_loss : 0.351812\n",
      "batch 3380 d_loss : 0.354645\n",
      "batch 3390 d_loss : 0.360981\n",
      "batch 3400 d_loss : 0.332916\n",
      "batch 3410 d_loss : 0.351508\n",
      "batch 3420 d_loss : 0.365650\n",
      "batch 3430 d_loss : 0.336136\n",
      "batch 3440 d_loss : 0.343418\n",
      "batch 3450 d_loss : 0.358650\n",
      "batch 3460 d_loss : 0.353471\n",
      "batch 3470 d_loss : 0.344380\n",
      "batch 3480 d_loss : 0.358385\n",
      "batch 3490 d_loss : 0.382730\n",
      "batch 3500 d_loss : 0.307405\n",
      "batch 3510 d_loss : 0.368548\n",
      "batch 3520 d_loss : 0.393361\n",
      "batch 3530 d_loss : 0.348213\n",
      "batch 3540 d_loss : 0.298813\n",
      "batch 3550 d_loss : 0.371003\n",
      "batch 3560 d_loss : 0.364556\n",
      "batch 3570 d_loss : 0.328037\n",
      "batch 3580 d_loss : 0.322700\n",
      "batch 3590 d_loss : 0.326398\n",
      "batch 3600 d_loss : 0.359382\n",
      "batch 3610 d_loss : 0.359920\n",
      "batch 3620 d_loss : 0.329384\n",
      "batch 3630 d_loss : 0.327072\n",
      "batch 3640 d_loss : 0.347410\n",
      "batch 3650 d_loss : 0.379413\n",
      "batch 3660 d_loss : 0.372703\n",
      "batch 3670 d_loss : 0.357675\n",
      "batch 3680 d_loss : 0.372805\n",
      "batch 3690 d_loss : 0.365402\n",
      "batch 3700 d_loss : 0.347396\n",
      "batch 3710 d_loss : 0.348621\n",
      "batch 3720 d_loss : 0.323398\n",
      "batch 3730 d_loss : 0.338582\n",
      "batch 3740 d_loss : 0.362022\n",
      "batch 3750 d_loss : 0.355821\n",
      "batch 3760 d_loss : 0.366558\n",
      "batch 3770 d_loss : 0.337374\n",
      "batch 3780 d_loss : 0.344690\n",
      "batch 3790 d_loss : 0.390551\n",
      "batch 3800 d_loss : 0.358623\n",
      "batch 3810 d_loss : 0.371677\n",
      "batch 3820 d_loss : 0.329747\n",
      "batch 3830 d_loss : 0.344297\n",
      "batch 3840 d_loss : 0.352936\n",
      "batch 3850 d_loss : 0.352679\n",
      "batch 3860 d_loss : 0.374000\n",
      "batch 3870 d_loss : 0.300268\n",
      "batch 3880 d_loss : 0.358812\n",
      "batch 3890 d_loss : 0.339392\n",
      "batch 3900 d_loss : 0.338168\n",
      "batch 3910 d_loss : 0.336316\n",
      "batch 3920 d_loss : 0.363233\n",
      "batch 3930 d_loss : 0.365039\n",
      "batch 3940 d_loss : 0.415401\n",
      "batch 3950 d_loss : 0.331191\n",
      "batch 3960 d_loss : 0.371664\n",
      "batch 3970 d_loss : 0.338527\n",
      "batch 3980 d_loss : 0.388488\n",
      "batch 3990 d_loss : 0.344702\n"
     ]
    }
   ],
   "source": [
    "train_GAN(4000,10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=100, units=1024)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), padding=\"same\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (5, 5), padding=\"same\")`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s     \n",
      "(150, 1, 128, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/anaconda2/envs/dlnd_gpu/lib/python3.6/site-packages/imageio/core/util.py:104: UserWarning: Conversion from int64 to uint8, range [0, 255]\n",
      "  'range [{2}, {3}]'.format(dtype_str, out_type.__name__, mi, ma))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "images = generate_image_batch(150)\n",
    "\n",
    "timestamp = str(int(time.time())) #for the image name so  the browser does not cache\n",
    "animation_file_name = \"./animation\"+timestamp+\".gif\"\n",
    "\n",
    "imageio.mimsave(animation_file_name,images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./animation1503561250.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.HTML('<img src=\"{}\">'.format(animation_file_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
